# LLM-Generated Contextual anomaly generation
import random
import torch
from torch_geometric.data import Data
from torch_geometric.utils import k_hop_subgraph
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from openai_query import send_query_to_openai
from .anomaly_list import ANOMALY_TYPE_LIST
from .utils import encode_text, count_label_occurrence
from typing import List
from .prompts import SYSTEM_PROMPT, USER_PROMPT_CORA, USER_PROMPT_CITESEER, USER_PROMPT_PUBMED, USER_PROMPT_ARXIV, USER_PROMPT_WIKICS
from collections import defaultdict
from typing import Dict

# ------------- PROMPT -------------

def llm_generated_contextual_anomaly_generator(data: Data, dataset_name: str, n: int, anomaly_type: int, random_seed: int, k_neighbors: int) -> Data:
    """
    Generate LLM-Generated Contextual anomaly for text-attributed graph
    """
    if ANOMALY_TYPE_LIST[anomaly_type] != "LLM-Generated Contextual Anomaly":
        raise ValueError(f"Invalid anomaly type: {anomaly_type}")
    if "cora" in dataset_name.lower():
        user_prompt = USER_PROMPT_CORA
    elif "citeseer" in dataset_name.lower():
        user_prompt = USER_PROMPT_CITESEER
    elif "pubmed" in dataset_name.lower():
        user_prompt = USER_PROMPT_PUBMED
    elif "arxiv" in dataset_name.lower():
        user_prompt = USER_PROMPT_ARXIV
    elif "wikics" in dataset_name.lower():
        user_prompt = USER_PROMPT_WIKICS
    else:
        raise ValueError(f"Dataset name: {dataset_name} is not implemented")
    return llm_anomaly_generation_pipeline(data, n, anomaly_type, random_seed, k_neighbors, user_prompt, SYSTEM_PROMPT)

# ------------ Pipeline ------------------
def llm_anomaly_generation_pipeline(data: Data, n: int, anomaly_type: int, random_seed: int, k_neighbors: int, user_prompt: str, system_prompt: str) -> Data:
    """
    The pipeline to generate LLM-Generated Contextual anomaly
    data must have the following attributes:
    .raw_texts: List[str] the original text attribute of nodes
    .category_names: List[str] the category/label of each node
    .label_names: List[str], all possible labels/categories
    .edge_index: torch.Tensor, shape: torch.Size([2, number of edges]), dtype: torch.int64

    Optional:
    .processed_text: List[str], the processed text attribute of nodes
    .anomaly_labels: torch.Tensor, shape: torch.Size([number of nodes]), dtype: torch.int64, the label of anomaly, 0 for normal, 1 for anomaly
    .anomaly_types: List[int], the type of anomaly
    .updated_x: torch.Tensor, shape: torch.Size([number of nodes, embedding_dim]), dtype: torch.float32, the updated text embeddings of nodes
    """
    raw_texts = data.raw_texts
    node_num = len(raw_texts)
    # get the index of normal nodes
    if hasattr(data, "anomaly_labels"):
        normal_idxs = (data.anomaly_labels == 0).nonzero(as_tuple=True)[0]
    else:
        normal_idxs = torch.arange(node_num)
    # if the number of normal nodes is less than n, raise error
    if normal_idxs.shape[0] < n:
        raise ValueError(f"The number of normal nodes is less than n: {normal_idxs.shape[0]} < {n}")
    # randomly select n nodes
    gen = torch.Generator(device=normal_idxs.device).manual_seed(int(random_seed))
    perm = torch.randperm(normal_idxs.numel(), generator=gen, device=normal_idxs.device)
    selected_idxs = normal_idxs[perm[:n]]
    # generate LLM-Generated Contextual anomaly
    data = llm_generated_contextual_anomaly(data, selected_idxs, anomaly_type, k_neighbors, user_prompt, system_prompt, random_seed)
    # encode the text to embeddings
    data = encode_text(data)
    # The updated embeddings should be stored in data.updated_x
    return data

# LLM-Generated Contextual anomaly generation
def llm_generated_contextual_anomaly(data: Data, selected_idxs: torch.Tensor, anomaly_type: int, k_neighbors: int, user_prompt: str, system_prompt: str, random_seed: int) -> Data:
    """
    Generate LLM-Generated Contextual anomaly
    replace the original text with the text generated by LLM
    """
    print("Generating LLM-Generated Contextual anomaly...")

    # step 1: initialize the processed text, anomaly labels, and anomaly types
    processed_text = data.raw_texts.copy() if not hasattr(data, "processed_text") else data.processed_text.copy()
    anomaly_labels = torch.zeros(len(processed_text), dtype=torch.int64, device=data.x.device) if not hasattr(data, "anomaly_labels") else data.anomaly_labels.clone()
    anomaly_types = [0] * len(processed_text) if not hasattr(data, "anomaly_types") else data.anomaly_types.copy()

    # step 2: replace the original text with the text generated by LLM
    count = 0
    for idx in selected_idxs.tolist():
        print(f"Generating LLM-Generated Contextual anomaly for node {idx}...")
        LLM_text = generate_LLM_text(data, idx, k_neighbors, user_prompt, system_prompt, random_seed)
        processed_text[idx] = LLM_text
        anomaly_labels[idx] = 1
        anomaly_types[idx] = anomaly_type
        count += 1
        if count % 100 == 0:
            print("--------------------------------")
            print(f"Generated {count} nodes")
            print("--------------------------------")

    # step 3: update the data
    data.processed_text = processed_text
    data.anomaly_labels = anomaly_labels
    data.anomaly_types = anomaly_types
    print("LLM-Generated Contextual anomaly generation completed")
    return data

# generate the text generated by LLM
def generate_LLM_text(data: Data, idx: int, k_neighbors: int, user_prompt: str, system_prompt: str, random_seed: int) -> str:
    """
    Generate the text generated by LLM
    """
    # step 1: get all label names
    label_names = data.label_names.copy()

    # step 2: delete the label name of node idx from label_names
    node_label = data.category_names[idx]
    if node_label in label_names:
        label_names.remove(node_label)
    
    # step 3: start from the first hop neighbor, delete the most appeared label name until only one label name is left
    for i in range(k_neighbors):
        neighbors = get_k_hop_neighbors(data, idx, i+1)
        # get the label names of the neighbors in the descending order of the count
        sorted_label_names = count_and_sort_label_names(data, neighbors)
        index = 0
        while (len(label_names) > 1 and index < len(sorted_label_names)):
            current_label_name = sorted_label_names[index]
            if current_label_name in label_names:
                label_names.remove(current_label_name)
            index += 1
        if len(label_names) == 1:
            break
        
    # step 4: Randomly select the label from the remaining label names based on the remaining label names' occurrence
    label_occurrence: Dict[str, int] = count_label_occurrence(data)
    # keep only the label in label_names
    label_occurrence = {label: occurrence for label, occurrence in label_occurrence.items() if label in label_names}
    # randomly select the label from the remaining label names based on the remaining label names' occurrence
    rng = random.Random(random_seed+idx)
    selected_label_name = rng.choices(list(label_occurrence.keys()), weights=list(label_occurrence.values()), k=1)[0]

    # step 5: formulate the prompt and send the query to OpenAI
    node_label_name = data.category_names[idx]
    raw_texts = data.raw_texts[idx]
    user_prompt = user_prompt.format(label_name=node_label_name, designated_label=selected_label_name, raw_text=raw_texts)
    print("="*100)
    print(f"\nUser prompt: {user_prompt}")
    return send_query_to_openai(user_prompt=user_prompt, system_prompt=system_prompt)

def get_k_hop_neighbors(data: Data, idx: int, k: int)-> List[int]:
    """
    Get the k-hop neighbors of the node
    """
    if k == 0:
        return []
    # get the k-hop subgraph
    subset_k, _, _, _ = k_hop_subgraph(idx, k, data.edge_index)
    if k == 1:
        exact_k = subset_k[subset_k != idx].tolist()
        return exact_k
    # get k-1 hop subgraph
    subset_k_1, _, _, _ = k_hop_subgraph(idx, k-1, data.edge_index)
    # get the neighbors of the node
    set_k   = set(subset_k.tolist())
    set_k_1 = set(subset_k_1.tolist())
    exact_k = list(set_k - set_k_1 - {idx})
    return exact_k


def count_and_sort_label_names(data: Data, neighbors: List[int]) -> List[str]:
    """
    Count the label name of the neighbors and sort the label names by the count
    Sort in descending order
    """
    label_name_count = defaultdict(int)
    for neighbor in neighbors:
        label_name_count[data.category_names[neighbor]] += 1
    sorted_label_name_count = sorted(label_name_count.items(), key=lambda x: x[1], reverse=True)
    return [label_name for label_name, _ in sorted_label_name_count]